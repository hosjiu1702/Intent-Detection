{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72107c71-453a-4eef-8689-d52b05a012a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PHOBERT FINE-TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b5711a-641c-46f2-8788-692e61d2eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a992012-357c-43b3-8ad3-c27512b23308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhosjiu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/emandai/intent-classifier/runs/s2pyy2lh\" target=\"_blank\">Fine-tuning</a></strong> to <a href=\"https://wandb.ai/emandai/intent-classifier\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/emandai/intent-classifier/runs/s2pyy2lh?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f1fda1beca0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" WANDB SETTINGS \"\"\"\n",
    "\n",
    "wandb.init(project=\"INTENT-CLASSIFIER\",\n",
    "           entity=\"emandai\",\n",
    "           name=\"Fine-tuning\",\n",
    "           save_code=True,\n",
    "           notes=\"Fine-tuning\",\n",
    "           tags=[\"fine-tune\", \"all-layer\", \"[TRAIN] no *other*\", \"[TEST] 100-calls\"],\n",
    "           config={\n",
    "               \"epochs\": 60,\n",
    "               \"lr\": 2e-5,\n",
    "               \"batch_size\": 16,\n",
    "               \"gradient_accumulation_steps\": 2,\n",
    "               \"weight_decay\": 0.01,\n",
    "               \"warmup_ratio\": 0.06,\n",
    "               \"lr_scheduler_type\": \"linear\"\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809b9556-8034-4132-8775-d865603fd436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" WANDB LOGIN \"\"\"\n",
    "\n",
    "wandb.login(relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c0c4923-0732-48d4-beec-0864e97dfa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "from pyvi import ViTokenizer\n",
    "from vncorenlp import VnCoreNLP\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from config import VQC_DATAPATH, phobert_base_checkpoint\n",
    "import utils\n",
    "from utils import count_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "442e28f7-4eb4-479d-909d-4bf11fe662e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" LOAD TOKENIZER \"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(phobert_base_checkpoint, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b554c191-1100-447e-b51d-7c8f69363e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [0, 10122, 29425, 14788, 46, 10578, 59, 8, 1701, 34412, 2],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TEST Tokenizer \"\"\"\n",
    "\n",
    "text = \"BÃ¡nh XÃ´i TiÃªu Ã´ng Máº«n ráº¥t lÃ  ngon.\"\n",
    "tokenized_text = tokenizer(text)\n",
    "pprint(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1923914-f278-4db7-b470-a012808c5f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> BÃ¡nh XÃ´i TiÃªu Ã´ng Máº«n ráº¥t lÃ  ngon. </s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TRY DECODE FROM LIST OF GIVEN TOKENS \"\"\"\n",
    "\n",
    "tokenizer.decode(tokenized_text[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1af5d45-ae5b-4c81-a2cd-d5c26c9be39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< s >'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Padding Token \"\"\"\n",
    "\n",
    "tokenizer.decode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a47ef8-29e9-485b-8e18-34dbc6a9b637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'< u n k >'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Unknown token \"\"\"\n",
    "\n",
    "tokenizer.decode(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a57c9c-6e23-4d95-9d7f-f67b7fb8485e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n",
      " 'input_ids': [0, 765, 1698, 17987, 57, 193, 2],\n",
      " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TEST Tokenizer \"\"\"\n",
    "\n",
    "text = \"kÃ½ ráº¹t Ä‘i em\"\n",
    "tokenized_text = tokenizer(text)\n",
    "pprint(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d64d8c3a-1a5f-437a-abb2-0a6f39d28a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 64001\n",
      "hidden size: 768\n",
      "num attention heads: 12\n",
      "num blocks: 12\n",
      "num labels: 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TRY TO LOAD MODEL CONFIGURATION \"\"\"\n",
    "\n",
    "model_config = AutoConfig.from_pretrained(phobert_base_checkpoint)\n",
    "print(f\"vocab size: {model_config.vocab_size}\")\n",
    "print(f\"hidden size: {model_config.hidden_size}\")\n",
    "print(f\"num attention heads: {model_config.num_attention_heads}\")\n",
    "print(f\"num blocks: {model_config.num_hidden_layers}\")\n",
    "print(f\"num labels: {model_config.num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2abe8-9425-40c1-9e51-03930653504d",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d19e42-2bad-463c-b045-14054d053cc3",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b0970c0-4852-48b5-b400-533cedd32283",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOAD DATA \"\"\"\n",
    "\n",
    "DATA_PATH = os.path.join(VQC_DATAPATH, \"data_no_other.xlsx\")\n",
    "data_df = pd.read_excel(DATA_PATH)\n",
    "data_df.head()\n",
    "\n",
    "X = data_df[\"Sample\"].tolist()\n",
    "y = data_df[\"Intent\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5060ac3-7b1f-40a2-b8f5-de4853215227",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92554a8d-6c65-47bb-a916-ae1fb69f8951",
   "metadata": {},
   "source": [
    "##### Text Lowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6188c44f-4dd5-42ea-ac76-0667ed5784dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TEXT LOWERING \"\"\"\n",
    "\n",
    "X = [text.lower() for text in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa76ee-3162-423e-9c8f-27a1c76846f7",
   "metadata": {},
   "source": [
    "##### Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eaa73fb-b725-441d-92b1-b71b0332c3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' WORD SEGMENTATION '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" WORD SEGMENTATION \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378426d5-ec01-401b-9d63-a5113d95b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyvi\n",
    "X = [ViTokenizer.tokenize(text) for text in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67fce080-6e26-4c27-9cbe-dcbf591a6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDRSegmenter\n",
    "annotator = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size=\"-Xmx500m\")\n",
    "\n",
    "def word_segmenter(f):\n",
    "    _concat = lambda x: \" \".join([token for token in x[0]])\n",
    "    def wrapper(*args, **kwargs):\n",
    "        list_of_tokens = f(*args, **kwargs)\n",
    "        return _concat(list_of_tokens)\n",
    "    return wrapper\n",
    "\n",
    "@word_segmenter\n",
    "def _ws(text):\n",
    "    return annotator.tokenize(text)\n",
    "\n",
    "X = [_ws(text) for text in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825c716-8aa3-4b55-b1dd-ebb22efea580",
   "metadata": {},
   "source": [
    "##### Label Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49b97c8e-3ce1-4841-9e0e-9aa0f5fa9ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verbose Labels]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" LABEL ENCODING (convert label from string to numeric data)\"\"\"\n",
    "\n",
    "lb = LabelEncoder()\n",
    "y = lb.fit_transform(y)\n",
    "print(\"[Verbose Labels]\")\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87a9c1fc-adaa-4d39-81c9-5fb3b36bce69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# samples: 1062\n"
     ]
    }
   ],
   "source": [
    "print(f\"# samples: {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fba33-6849-4767-b4dd-5e5111952a89",
   "metadata": {},
   "source": [
    "##### (Train, Val, Test) Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20a60fcb-312e-403d-96ae-142aefb87050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DATA SPLITING FOR TRAINING, EVALUATING AND TESTING \"\"\"\n",
    "\n",
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE = 0.1\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(X, y, test_size=TEST_SIZE, random_state=42)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=VAL_SIZE, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9baac8f1-1510-4486-aad3-5933928d5a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 39, 38)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(train_labels)), len(np.unique(val_labels)), len(np.unique(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc4518-f667-4f6a-83ba-d914fcb08b8b",
   "metadata": {},
   "source": [
    "##### Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5eaaa53-b49d-4295-8a42-34410fc6f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TOKENIZE FOR EACH SPLIT \"\"\"\n",
    "\n",
    "train_encodings = tokenizer(train_texts, padding=\"max_length\", truncation=True)\n",
    "val_encodings = tokenizer(val_texts, padding=\"max_length\", truncation=True)\n",
    "test_encodings = tokenizer(test_texts, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1991f-bfee-47fa-853c-49d772955869",
   "metadata": {},
   "source": [
    "#### Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bb1d516-e9cc-4517-93e9-8e18bd97ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CREATE DATASET (including `encodings` and `labels`) \"\"\"\n",
    "\n",
    "class VqcDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        for key, val in self.encodings.items():\n",
    "            item.update({key: torch.tensor(val[idx])})\n",
    "        item.update({\"label\": torch.tensor(self.labels[idx])})\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    # @classmethod\n",
    "    def get_num_labels(self):\n",
    "        return len(np.unique(self.labels))\n",
    "\n",
    "train_dataset = VqcDataset(train_encodings, train_labels)\n",
    "val_dataset = VqcDataset(val_encodings, val_labels)\n",
    "test_dataset = VqcDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d5a5e-9fe8-4c8d-824f-7afcf34597f5",
   "metadata": {},
   "source": [
    "### Load Pre-trained PhoBERT-base checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ac00857-b9a1-43ca-bdf0-064b210026a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./results/TAPT/checkpoint-2500 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./results/TAPT/checkpoint-2500 and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‡ MODEL CONFIGURATIONS ðŸ‘‡\n",
      "\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"./results/TAPT/checkpoint-2500\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" LOAD PRE-TRAINED PhoBERT MODEL \"\"\"\n",
    "\n",
    "# We can use RobertaForSequenceClassification as an alternative\n",
    "num_labels = train_dataset.get_num_labels()\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./results/TAPT/checkpoint-2500\",\n",
    "                                                           num_labels=num_labels)\n",
    "\n",
    "# Re-check model configurations\n",
    "print(f\"ðŸ‘‡ MODEL CONFIGURATIONS ðŸ‘‡\\n\")\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd3230b5-636d-4015-ae34-e89a8744626a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./results/pretraining/checkpoint-100000/ were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./results/pretraining/checkpoint-100000/ and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" LOAD PRE-TRAINED PhoBERT MODEL \"\"\"\n",
    "\n",
    "# We can use RobertaForSequenceClassification as an alternative\n",
    "num_labels = train_dataset.get_num_labels()\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./results/pretraining/checkpoint-100000/\",\n",
    "                                                           num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89f228-4373-440a-b373-38d6f853db58",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ab03140-5ca7-4872-afa2-dec0c7f7cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Metric for model evaluation\n",
    "# Reference: https://huggingface.co/transformers/training.html#fine-tuning-in-pytorch-with-the-trainer-api\n",
    "metric = load_metric(\"f1\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=predictions,\n",
    "                          references=labels,\n",
    "                          average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9287c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"INTENT-CLASSIFIER\",\n",
    "           entity=\"emandai\",\n",
    "           name=\"Fine-tuning\",\n",
    "           save_code=True,\n",
    "           notes=\"Fine-tuning\",\n",
    "           tags=[\"fine-tune\", \"all-layer\", \"[TRAIN] no *other*\", \"[TEST] 100-calls\"],\n",
    "           config={\n",
    "               \"epochs\": 60,\n",
    "               \"lr\": 2e-5,\n",
    "               \"batch_size\": 16,\n",
    "               \"gradient_accumulation_steps\": 2,\n",
    "               \"weight_decay\": 0.01,\n",
    "               \"warmup_ratio\": 0.06,\n",
    "               \"lr_scheduler_type\": \"linear\"\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec79ed3a-311c-4283-bf7e-14a36829044d",
   "metadata": {},
   "source": [
    "### Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "797dca31-c6e6-470b-9379-3b53623c4602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Argument\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=wandb.config.epochs,\n",
    "    learning_rate=wandb.config.lr,\n",
    "    per_device_train_batch_size=wandb.config.batch_size,\n",
    "    per_device_eval_batch_size=wandb.config.batch_size,\n",
    "    gradient_accumulation_steps=wandb.config.gradient_accumulation_steps,\n",
    "    weight_decay=wandb.config.weight_decay,\n",
    "    warmup_ratio=wandb.config.warmup_ratio,\n",
    "    lr_scheduler_type=wandb.config.lr_scheduler_type,\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    save_total_limit=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "# Fine-tuning using Trainer API from Huggingface\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(5)] # Early Stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e27608-5077-4c12-addb-da20056ee2b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine-tuning All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d78542c1-67fd-4805-a18e-f7e29192b3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 859\n",
      "  Num Epochs = 60\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1620\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of trainable parameters: 154714410\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1620/1620 09:38, Epoch 60/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.359600</td>\n",
       "      <td>3.296652</td>\n",
       "      <td>0.246738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.384200</td>\n",
       "      <td>2.359263</td>\n",
       "      <td>0.476249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.598000</td>\n",
       "      <td>1.673203</td>\n",
       "      <td>0.649381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.037600</td>\n",
       "      <td>1.249442</td>\n",
       "      <td>0.691361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.599500</td>\n",
       "      <td>1.019155</td>\n",
       "      <td>0.742193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.862205</td>\n",
       "      <td>0.731624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.241800</td>\n",
       "      <td>0.809447</td>\n",
       "      <td>0.725590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.807439</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.806648</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>0.808311</td>\n",
       "      <td>0.714750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.809232</td>\n",
       "      <td>0.714750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.804379</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.821179</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.816567</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.061000</td>\n",
       "      <td>0.822738</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.824428</td>\n",
       "      <td>0.725782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-100\n",
      "Configuration saved in ./results/checkpoint-100/config.json\n",
      "Model weights saved in ./results/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-200\n",
      "Configuration saved in ./results/checkpoint-200/config.json\n",
      "Model weights saved in ./results/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-300\n",
      "Configuration saved in ./results/checkpoint-300/config.json\n",
      "Model weights saved in ./results/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-400\n",
      "Configuration saved in ./results/checkpoint-400/config.json\n",
      "Model weights saved in ./results/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-600\n",
      "Configuration saved in ./results/checkpoint-600/config.json\n",
      "Model weights saved in ./results/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-700\n",
      "Configuration saved in ./results/checkpoint-700/config.json\n",
      "Model weights saved in ./results/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-800\n",
      "Configuration saved in ./results/checkpoint-800/config.json\n",
      "Model weights saved in ./results/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-900\n",
      "Configuration saved in ./results/checkpoint-900/config.json\n",
      "Model weights saved in ./results/checkpoint-900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1100\n",
      "Configuration saved in ./results/checkpoint-1100/config.json\n",
      "Model weights saved in ./results/checkpoint-1100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1200\n",
      "Configuration saved in ./results/checkpoint-1200/config.json\n",
      "Model weights saved in ./results/checkpoint-1200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1300\n",
      "Configuration saved in ./results/checkpoint-1300/config.json\n",
      "Model weights saved in ./results/checkpoint-1300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1400\n",
      "Configuration saved in ./results/checkpoint-1400/config.json\n",
      "Model weights saved in ./results/checkpoint-1400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1600\n",
      "Configuration saved in ./results/checkpoint-1600/config.json\n",
      "Model weights saved in ./results/checkpoint-1600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-1200 (score: 0.8043792843818665).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish, PID 1479... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 4.82MB of 4.82MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">Fine-tuning</strong>: <a href=\"https://wandb.ai/emandai/intent-classifier/runs/s2pyy2lh\" target=\"_blank\">https://wandb.ai/emandai/intent-classifier/runs/s2pyy2lh</a><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"total number of trainable parameters: {count_trainable_params(model)}\")\n",
    "\n",
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c224f6f-9d70-49e3-80a2-0a828e2a156e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fine-tuning Last Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a5519-4598-4067-94ea-71989d9ed0f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For embedding layers\n",
    "for name, params in model.roberta.embeddings.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# For encoder layers\n",
    "for name, params in model.roberta.encoder.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "    if \"layer.11\" in name:\n",
    "        params.requires_grad = True\n",
    "\n",
    "print(f\"total number of trainable parameters: {count_trainable_params(model)}\")\n",
    "        \n",
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a5339-c012-4b9a-8e30-d3215ee82507",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fine-tune from 11th Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c979a1d7-2792-44da-8323-e77cf0a117cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For embedding layers\n",
    "for name, params in model.roberta.embeddings.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# For encoder layers\n",
    "for name, params in model.roberta.encoder.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "    if \"layer.10\" in name:\n",
    "        params.requires_grad = True\n",
    "        continue\n",
    "    \n",
    "    if \"layer.11\" in name:\n",
    "        params.requires_grad = True\n",
    "\n",
    "print(f\"total number of trainable parameters: {count_trainable_params(model)}\")\n",
    "        \n",
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18d4e90-1299-4a9c-816e-9f59eeeea048",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fine-tune from 10th Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc68c2d-0c0f-4ae5-8820-b4d6646da1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For embedding layers\n",
    "for name, params in model.roberta.embeddings.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# For encoder layers\n",
    "for name, params in model.roberta.encoder.named_parameters():\n",
    "    params.requires_grad = False\n",
    "    \n",
    "    if \"layer.9\" in name:\n",
    "        params.requires_grad = True\n",
    "        continue\n",
    "    \n",
    "    if \"layer.10\" in name:\n",
    "        params.requires_grad = True\n",
    "        continue\n",
    "    \n",
    "    if \"layer.11\" in name:\n",
    "        params.requires_grad = True\n",
    "\n",
    "print(f\"total number of trainable parameters: {sum([params.numel() for params in model.parameters() if params.requires_grad])}\")\n",
    "        \n",
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800575d5-8b19-4579-88ab-28ff2b585c03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Fine-tune from 9th Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32a07b95-83ca-40fe-81ba-ce24fe6a07bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 859\n",
      "  Num Epochs = 60\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1620\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of trainable parameters: 28974378\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1620' max='1620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1620/1620 05:09, Epoch 60/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.390400</td>\n",
       "      <td>3.288343</td>\n",
       "      <td>0.277443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.372000</td>\n",
       "      <td>2.308057</td>\n",
       "      <td>0.461140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.705400</td>\n",
       "      <td>1.707848</td>\n",
       "      <td>0.601507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.251700</td>\n",
       "      <td>1.327790</td>\n",
       "      <td>0.695901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.844300</td>\n",
       "      <td>1.087066</td>\n",
       "      <td>0.735015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.591400</td>\n",
       "      <td>0.915444</td>\n",
       "      <td>0.750985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.468100</td>\n",
       "      <td>0.815914</td>\n",
       "      <td>0.772123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.319700</td>\n",
       "      <td>0.750850</td>\n",
       "      <td>0.782963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.248800</td>\n",
       "      <td>0.711245</td>\n",
       "      <td>0.782963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>0.693392</td>\n",
       "      <td>0.773872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.172700</td>\n",
       "      <td>0.676533</td>\n",
       "      <td>0.768065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.130900</td>\n",
       "      <td>0.670143</td>\n",
       "      <td>0.777156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.663134</td>\n",
       "      <td>0.768065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.108100</td>\n",
       "      <td>0.659656</td>\n",
       "      <td>0.797267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.107600</td>\n",
       "      <td>0.665430</td>\n",
       "      <td>0.797267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.667298</td>\n",
       "      <td>0.797267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-100\n",
      "Configuration saved in ./results/checkpoint-100/config.json\n",
      "Model weights saved in ./results/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-200\n",
      "Configuration saved in ./results/checkpoint-200/config.json\n",
      "Model weights saved in ./results/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-300\n",
      "Configuration saved in ./results/checkpoint-300/config.json\n",
      "Model weights saved in ./results/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-400\n",
      "Configuration saved in ./results/checkpoint-400/config.json\n",
      "Model weights saved in ./results/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-600\n",
      "Configuration saved in ./results/checkpoint-600/config.json\n",
      "Model weights saved in ./results/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-700\n",
      "Configuration saved in ./results/checkpoint-700/config.json\n",
      "Model weights saved in ./results/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-800\n",
      "Configuration saved in ./results/checkpoint-800/config.json\n",
      "Model weights saved in ./results/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-900\n",
      "Configuration saved in ./results/checkpoint-900/config.json\n",
      "Model weights saved in ./results/checkpoint-900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1100\n",
      "Configuration saved in ./results/checkpoint-1100/config.json\n",
      "Model weights saved in ./results/checkpoint-1100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1200\n",
      "Configuration saved in ./results/checkpoint-1200/config.json\n",
      "Model weights saved in ./results/checkpoint-1200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1300\n",
      "Configuration saved in ./results/checkpoint-1300/config.json\n",
      "Model weights saved in ./results/checkpoint-1300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1400\n",
      "Configuration saved in ./results/checkpoint-1400/config.json\n",
      "Model weights saved in ./results/checkpoint-1400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-1600\n",
      "Configuration saved in ./results/checkpoint-1600/config.json\n",
      "Model weights saved in ./results/checkpoint-1600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-1400 (score: 0.6596562266349792).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 96\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish, PID 4103763... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 4.96MB of 4.96MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">Fine-tuning</strong>: <a href=\"https://wandb.ai/emandai/intent-classifier/runs/380g906f\" target=\"_blank\">https://wandb.ai/emandai/intent-classifier/runs/380g906f</a><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For embedding layers\n",
    "for name, params in model.roberta.embeddings.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# For encoder layers\n",
    "for name, params in model.roberta.encoder.named_parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "    if \"layer.8\" in name:\n",
    "        params.requires_grad = True\n",
    "        continue\n",
    "    \n",
    "    if \"layer.9\" in name:\n",
    "        params.requires_grad = True\n",
    "        continue\n",
    "    \n",
    "    if \"layer.10\" in name:\n",
    "        params.requires_grad = True\n",
    "        continue\n",
    "    \n",
    "    if \"layer.11\" in name:\n",
    "        params.requires_grad = True\n",
    "\n",
    "print(f\"total number of trainable parameters: {count_trainable_params(model)}\")\n",
    "        \n",
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# evaluate\n",
    "trainer.evaluate()\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4132f2e9-757e-44e3-811b-6c392c566414",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Fine-tuning Classification Head Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77749e-4195-4529-8a05-0e4e2f966405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        params.requires_grad = False\n",
    "\n",
    "print(f\"total number of trainable parameters: {count_trainable_params(model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94139883-47a3-4006-a95c-ca6608779ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Finish wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ffdd36-d1ce-4274-818c-2dcb578ae8c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebafd4a9-15c5-4172-aade-6ac88f4d2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DEFINE EVALUATE FUNCTION \"\"\"\n",
    "\n",
    "def evaluate(texts, labels, metric=\"f1\"):\n",
    "    preds = []\n",
    "    for text in texts:\n",
    "        input_ids = torch.tensor([tokenizer.encode(text)]).to(device=\"cuda:0\")\n",
    "        logits = model(input_ids).logits\n",
    "        prob = torch.softmax(logits, dim=1)\n",
    "        max_idx = torch.argmax(prob).item()\n",
    "        preds.append(max_idx)\n",
    "\n",
    "    if metric == \"f1\":\n",
    "        f1 = f1_score(labels, preds, average=\"macro\")\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "424d3ac6-881e-4eea-9647-631850fcddd2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro: 0.7495765929976458\n"
     ]
    }
   ],
   "source": [
    "\"\"\" EVALUATE ON TEST SPLIT \"\"\"\n",
    "\n",
    "model.eval()\n",
    "f1 = evaluate(test_texts, test_labels, metric=\"f1\")\n",
    "print(f\"F1-macro: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "149d6b23-c6fb-45e1-a134-c9ebc3a0169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOAD REAL TEST DATA \"\"\"\n",
    "\n",
    "X_test, y_test = utils.get_test(target=\"single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8607119-63fd-447f-ae2b-f0bb9a029dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SEGMENT REAL TEST DATA \"\"\"\n",
    "\n",
    "X_test = [_ws(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb432b6e-f761-46f4-ac39-7c58a63d79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LABEL ENCODING FOR REAL TEST DATA \"\"\"\n",
    "\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1eb4c8c-7aba-4c4b-a299-145132ab1251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-macro: 0.4225098806151392\n"
     ]
    }
   ],
   "source": [
    "\"\"\" EVALUATE ON REAL TEST SET \"\"\"\n",
    "\n",
    "model.eval()\n",
    "f1 = evaluate(X_test, y_test, metric=\"f1\")\n",
    "print(f\"F1-macro: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25bcb83-ab27-4d6f-b0ce-f1d4d330d64e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" CONFUSION MATRIX [PhoBERT Fine-Tuning] \"\"\"\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "print(len(np.unique(preds)))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf070d-6764-4750-93ca-88ee3bae8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" INFERENCE TESTING \"\"\"\n",
    "\n",
    "text = \"á»« em Ä‘Ã³ng ráº¥t lÃ  tá»‘t em Ä‘Ã³ng ráº¥t lÃ  tá»‘t vÃ  cÃ³ uy_tÃ­n cá»§a cÃ´ng_ty chá»‹ cho_nÃªn cÃ´ng_ty chá»‹ Ä‘á»£t nÃ y má»›i Æ°u_Ä‘Ã£i láº¡i cho em má»™t á» cÃ¡i khoáº£n vay báº±ng tiá»n_máº·t\"\n",
    "\n",
    "# encode = tokenize + numericalize\n",
    "input_ids = torch.tensor([tokenizer.encode(text)]).to(device=\"cuda:0\")\n",
    "\n",
    "# forward\n",
    "logit = model(input_ids).logits\n",
    "print(\"Output Logits:\\n\")\n",
    "print(logit, end=\"\\n\\n\")\n",
    "\n",
    "print(\"Output Labels:\")\n",
    "prob = torch.softmax(logit, dim=1)\n",
    "max_idx = torch.argmax(prob).item()\n",
    "lb.inverse_transform(np.array([max_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e63fd-0062-45e3-a1f7-98586d457aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[1000], lb.inverse_transform([y_test[1000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e6025-7837-4d3c-bead-d978c5cf314c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TASK ADAPTIVE PRE-TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cfa24d67-1df9-4534-a904-34c62bddb9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import wandb\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from vncorenlp import VnCoreNLP\n",
    "from pyvi import ViTokenizer\n",
    "from joblib import Parallel, delayed\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "from config import (\n",
    "    VQC_UNLABELED_DATAPATH,\n",
    "    VQC_DATAPATH,\n",
    "    phobert_base_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2666c086-8fa3-49f7-aa6f-3933e0bf0a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "transformers.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37b6735e-c5ff-4125-be03-f846dbca6ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/emandai/intent-classifier/runs/3pblif3k\" target=\"_blank\">Task Adaptive Pretraining</a></strong> to <a href=\"https://wandb.ai/emandai/intent-classifier\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/emandai/intent-classifier/runs/3pblif3k?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa94162db50>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" WANDB SETTINGS \"\"\"\n",
    "\n",
    "wandb.init(project=\"INTENT-CLASSIFIER\",\n",
    "           entity=\"emandai\",\n",
    "           name=\"Task Adaptive Pretraining\",\n",
    "           save_code=True,\n",
    "           tags=[\"pretraining\", \"TAPT\"],\n",
    "           config={\n",
    "               \"epochs\": 100,\n",
    "               \"lr\": 2e-5,\n",
    "               \"batch_size\": 8,\n",
    "               \"gradient_accumulation_steps\": 2,\n",
    "               \"weight_decay\": 0.01,\n",
    "               \"warmup_ratio\": 0.06,\n",
    "               \"lr_scheduler_type\": \"linear\"\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b728716-16ca-4d41-b6cb-eedb771bd703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /home/kiethoang/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/vinai/phobert-base/resolve/main/pytorch_model.bin from cache at /home/kiethoang/.cache/huggingface/transformers/8363542cfd9e2bad1a9a618e87ea1153d84819a3ae581cff0816a2c1f610f433.42a5e558f15db4cc3af338445707272b8f7545df78efdc125d3fd51025b22d85\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at vinai/phobert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" LOAD PHOBERT CHECKPOINT \"\"\"\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(phobert_base_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461f387-bd63-4fc8-9659-cadd4619fe0e",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14fcd48c-3e50-4573-965b-ada8a120a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" LOAD TRANSCRIBED TEXTS \"\"\"\n",
    "\n",
    "# path = os.path.join(VQC_UNLABELED_DATAPATH, \"transcribed_texts.txt\")\n",
    "# lines_df = pd.read_csv(path, delimiter=\"\\n\", names=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3503c709-88d6-4393-8de0-8cd7ea962bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anh lÃ  Ã‚n Ä‘Ã¢y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chá»‹ lÃ  hiáº¿u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>em tÃªn lÃ  hÃ </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em hÃ  Ä‘Ã¢y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>em lÃ  hÆ°Æ¡ng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sample\n",
       "0  anh lÃ  Ã‚n Ä‘Ã¢y\n",
       "1    chá»‹ lÃ  hiáº¿u\n",
       "2   em tÃªn lÃ  hÃ \n",
       "3      em hÃ  Ä‘Ã¢y\n",
       "4    em lÃ  hÆ°Æ¡ng"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = os.path.join(VQC_DATAPATH, \"pretrain_data_medium.xlsx\")\n",
    "\n",
    "lines_df = pd.read_excel(DATA_PATH, usecols=[\"Sample\"])\n",
    "lines_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494bb526-f694-43d2-b939-ccce8780dc95",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa2e09-e9ed-49d7-8cdf-1e0680ea79b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Remove Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7188aa14-99cd-43fc-93e4-084ca15ee8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anh lÃ  Ã‚n Ä‘Ã¢y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chá»‹ lÃ  hiáº¿u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>em tÃªn lÃ  hÃ </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>em hÃ  Ä‘Ã¢y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>em lÃ  hÆ°Æ¡ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2584</th>\n",
       "      <td>Ä‘á»£t nÃ y bÃªn em cÃ³ má»Ÿ ra má»™t cÃ¡i gÃ³i vay vá»‘n ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>mÃ  khÃ´ng biáº¿t lÃ  chá»‹ cÃ¢n nháº¯c Ä‘Æ°á»£c khoáº£n nÃ o k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>thÃ¬ bÃªn em há»— trá»£ cho mÃ¬nh nháº­n vá» thÃ¬ sá»‘ khoáº£...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587</th>\n",
       "      <td>á»§a giá» nhÆ° dá»‹ch bá»‡nh nÃ y mÃ  lá»¡ may khÃ´ng cÃ³ ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>á»§a mÃ  mÃ  mÃ  bÃ¢y giá» tui Ä‘ang á»Ÿ trong khu cÃ¡ch ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2589 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sample\n",
       "0                                         anh lÃ  Ã‚n Ä‘Ã¢y\n",
       "1                                           chá»‹ lÃ  hiáº¿u\n",
       "2                                          em tÃªn lÃ  hÃ \n",
       "3                                             em hÃ  Ä‘Ã¢y\n",
       "4                                           em lÃ  hÆ°Æ¡ng\n",
       "...                                                 ...\n",
       "2584  Ä‘á»£t nÃ y bÃªn em cÃ³ má»Ÿ ra má»™t cÃ¡i gÃ³i vay vá»‘n ti...\n",
       "2585  mÃ  khÃ´ng biáº¿t lÃ  chá»‹ cÃ¢n nháº¯c Ä‘Æ°á»£c khoáº£n nÃ o k...\n",
       "2586  thÃ¬ bÃªn em há»— trá»£ cho mÃ¬nh nháº­n vá» thÃ¬ sá»‘ khoáº£...\n",
       "2587  á»§a giá» nhÆ° dá»‹ch bá»‡nh nÃ y mÃ  lá»¡ may khÃ´ng cÃ³ ti...\n",
       "2588  á»§a mÃ  mÃ  mÃ  bÃ¢y giá» tui Ä‘ang á»Ÿ trong khu cÃ¡ch ...\n",
       "\n",
       "[2589 rows x 1 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" REMOVE DUPLICATE \"\"\"\n",
    "\n",
    "lines_df.drop_duplicates(ignore_index=True, inplace=True)\n",
    "lines_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e6d5e-c899-431d-9a13-bcd48f1ee6aa",
   "metadata": {},
   "source": [
    "#### Text Lowering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "619c97eb-16f7-43d7-a73a-1b1173c8612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TEXT LOWERING \"\"\"\n",
    "\n",
    "for index, row in lines_df.iterrows():\n",
    "    lines_df.at[index, \"Sample\"] = row[\"Sample\"].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c182f-36a4-442d-8fb4-16a53885e9b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72f4cb-693c-4042-b71c-1c58cda665b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pyvi\n",
    "ws_texts = Parallel(n_jobs=8)(delayed(ViTokenizer.tokenize)(row[\"text\"]) for _, row in lines_df.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a44dc084-e2fe-4613-9d48-11756c28e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDRSegmenter\n",
    "annotator = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size=\"-Xmx500m\")\n",
    "\n",
    "def word_segmenter(f):\n",
    "    _concat = lambda x: \" \".join([token for token in x[0]])\n",
    "    def wrapper(*args, **kwargs):\n",
    "        list_of_tokens = f(*args, **kwargs)\n",
    "        return _concat(list_of_tokens)\n",
    "    return wrapper\n",
    "\n",
    "@word_segmenter\n",
    "def _ws(text):\n",
    "    return annotator.tokenize(text)\n",
    "\n",
    "\n",
    "ws_texts = [_ws(row[\"Sample\"]) for _, row in lines_df.iterrows()]\n",
    "for index, row in lines_df.iterrows():\n",
    "    lines_df.at[index, \"Sample\"] = _ws(row[\"Sample\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9199adaf-11e7-45d6-a835-dfd5629a5727",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e3e99e75-b289-46ed-b44a-e8b0b34f0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" INITSTANTIATE HF DATASET FROM PANDAS DATAFRAME \"\"\"\n",
    "dataset = Dataset.from_pandas(df=lines_df)\n",
    "\n",
    "\"\"\" TRAIN TEST SPLIT \"\"\"\n",
    "TEST_SIZE = 0.1\n",
    "dataset = dataset.train_test_split(test_size=TEST_SIZE)\n",
    "dataset[\"validation\"] = dataset.pop(\"test\") # For name consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c834d-2056-4362-b683-86a4d0e132fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Text Tokenization (removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c70fcf6a-5dda-4ecd-8e78-89f799950fe3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /home/kiethoang/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/vocab.txt from cache at /home/kiethoang/.cache/huggingface/transformers/970c6224b2713c8b52a7bcfc4d5a951c9bb88302e4523388b50f28284e87ac44.26ba0c8945e559c68d0bc35d24fea16f5463a49fe8f134e0c32261d590b577fa\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/bpe.codes from cache at /home/kiethoang/.cache/huggingface/transformers/f3a66ae0a78d1a53b3eb99e31837d0d8e2f684a2dcc1f52f75fd36873e3d79de.301ac8958de708ddcea8500d9acbe6261dba391d249c98dcda1e49dbbff870dd\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /home/kiethoang/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(phobert_base_checkpoint, use_fast=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "10743847-6be9-41f2-b451-e827cd872082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" ADD NEW TOKENS TO VOCAB \"\"\"\n",
    "\n",
    "# # build vocab from the train set\n",
    "# def yield_tokens_from_hf_dataset(dataset):\n",
    "#     for text in dataset[\"text\"]:\n",
    "#         yield text.split()\n",
    "# vocab = build_vocab_from_iterator(iterator=yield_tokens_from_hf_dataset(dataset[\"train\"]))\n",
    "\n",
    "# # add new tokens to the tokenizer\n",
    "# num_added_tokens = tokenizer.add_tokens(list(vocab.stoi)[2:]) # remove <pad> and <unk> token\n",
    "\n",
    "# print(f\"[BEFORE] vocab size: {tokenizer.vocab_size}\")\n",
    "# print(f\"[AFTER] vocab size: {len(tokenizer)} (+{len(tokenizer) - tokenizer.vocab_size})\")\n",
    "\n",
    "# # Update embeddings matrix size\n",
    "# # model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b343c5d-36db-456a-b376-82377c7211e4",
   "metadata": {},
   "source": [
    "### Training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "939edad1-4de9-4a13-b0d8-011fbb77bf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TRAINING SETTINGS \"\"\"\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    num_train_epochs=wandb.config.epochs,\n",
    "    learning_rate=wandb.config.lr,\n",
    "    lr_scheduler_type=wandb.config.lr_scheduler_type,\n",
    "    weight_decay=wandb.config.weight_decay,\n",
    "    warmup_ratio=wandb.config.warmup_ratio,\n",
    "    per_device_train_batch_size=wandb.config.batch_size,\n",
    "    per_device_eval_batch_size=wandb.config.batch_size,\n",
    "    gradient_accumulation_steps=wandb.config.gradient_accumulation_steps,\n",
    "    logging_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    logging_dir=\"./logs/TAPT\",\n",
    "    output_dir=\"./results/TAPT\",\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "# Random mask\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6f5ea-46bc-4dc6-8f2b-c85328fa5dd8",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b72e2fa-6b4d-40f2-8ed6-bfd190a80328",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2330\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 14600\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='14600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3000/14600 15:08 < 58:35, 3.30 it/s, Epoch 20/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.416700</td>\n",
       "      <td>4.510520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.942100</td>\n",
       "      <td>3.764014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.941400</td>\n",
       "      <td>3.510015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.155700</td>\n",
       "      <td>3.038373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.808900</td>\n",
       "      <td>2.948938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.861100</td>\n",
       "      <td>2.961706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.770300</td>\n",
       "      <td>2.701191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.451000</td>\n",
       "      <td>2.583792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.524000</td>\n",
       "      <td>2.395486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.741400</td>\n",
       "      <td>2.558349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.564300</td>\n",
       "      <td>2.545668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.993200</td>\n",
       "      <td>2.402087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.338800</td>\n",
       "      <td>2.340636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.274600</td>\n",
       "      <td>2.393654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.043600</td>\n",
       "      <td>2.426581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.271800</td>\n",
       "      <td>2.335215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.266500</td>\n",
       "      <td>2.396554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.027600</td>\n",
       "      <td>2.370804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.800100</td>\n",
       "      <td>2.314512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.820600</td>\n",
       "      <td>2.294625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.910500</td>\n",
       "      <td>2.371946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.856300</td>\n",
       "      <td>2.167325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.939800</td>\n",
       "      <td>2.182088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.707300</td>\n",
       "      <td>2.302521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.636600</td>\n",
       "      <td>2.090914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.774400</td>\n",
       "      <td>2.144854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.565500</td>\n",
       "      <td>2.353202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.139600</td>\n",
       "      <td>2.402102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.787500</td>\n",
       "      <td>2.258165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.935400</td>\n",
       "      <td>2.241846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-100\n",
      "Configuration saved in ./results/TAPT/checkpoint-100/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-200\n",
      "Configuration saved in ./results/TAPT/checkpoint-200/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-300\n",
      "Configuration saved in ./results/TAPT/checkpoint-300/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-400\n",
      "Configuration saved in ./results/TAPT/checkpoint-400/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-500\n",
      "Configuration saved in ./results/TAPT/checkpoint-500/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-600\n",
      "Configuration saved in ./results/TAPT/checkpoint-600/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-700\n",
      "Configuration saved in ./results/TAPT/checkpoint-700/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-800\n",
      "Configuration saved in ./results/TAPT/checkpoint-800/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-900\n",
      "Configuration saved in ./results/TAPT/checkpoint-900/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1000\n",
      "Configuration saved in ./results/TAPT/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1100\n",
      "Configuration saved in ./results/TAPT/checkpoint-1100/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1200\n",
      "Configuration saved in ./results/TAPT/checkpoint-1200/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1300\n",
      "Configuration saved in ./results/TAPT/checkpoint-1300/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1400\n",
      "Configuration saved in ./results/TAPT/checkpoint-1400/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1500\n",
      "Configuration saved in ./results/TAPT/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1600\n",
      "Configuration saved in ./results/TAPT/checkpoint-1600/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1700\n",
      "Configuration saved in ./results/TAPT/checkpoint-1700/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1800\n",
      "Configuration saved in ./results/TAPT/checkpoint-1800/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-1900\n",
      "Configuration saved in ./results/TAPT/checkpoint-1900/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-1900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2000\n",
      "Configuration saved in ./results/TAPT/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2100\n",
      "Configuration saved in ./results/TAPT/checkpoint-2100/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2100/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2200\n",
      "Configuration saved in ./results/TAPT/checkpoint-2200/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2200/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2300\n",
      "Configuration saved in ./results/TAPT/checkpoint-2300/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2300/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2400\n",
      "Configuration saved in ./results/TAPT/checkpoint-2400/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2400/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2500\n",
      "Configuration saved in ./results/TAPT/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2600\n",
      "Configuration saved in ./results/TAPT/checkpoint-2600/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2600/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2700\n",
      "Configuration saved in ./results/TAPT/checkpoint-2700/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2700/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2800\n",
      "Configuration saved in ./results/TAPT/checkpoint-2800/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2800/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-2900\n",
      "Configuration saved in ./results/TAPT/checkpoint-2900/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-2900/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-1900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TAPT/checkpoint-3000\n",
      "Configuration saved in ./results/TAPT/checkpoint-3000/config.json\n",
      "Model weights saved in ./results/TAPT/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/TAPT/checkpoint-2000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/TAPT/checkpoint-2500 (score: 2.090914487838745).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish, PID 4083292... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 12.69MB of 12.69MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">Task Adaptive Pretraining</strong>: <a href=\"https://wandb.ai/emandai/intent-classifier/runs/3pblif3k\" target=\"_blank\">https://wandb.ai/emandai/intent-classifier/runs/3pblif3k</a><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "wandb.finish(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8208c95-5050-43ee-b0fc-5d1a382b1d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "40c3116b-5d27-49fd-ad7d-c02024a4268c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./results/TAPT/checkpoint-2500/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"vinai/phobert-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file ./results/TAPT/checkpoint-2500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ./results/TAPT/checkpoint-2500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /home/kiethoang/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/vinai/phobert-base/resolve/main/pytorch_model.bin from cache at /home/kiethoang/.cache/huggingface/transformers/8363542cfd9e2bad1a9a618e87ea1153d84819a3ae581cff0816a2c1f610f433.42a5e558f15db4cc3af338445707272b8f7545df78efdc125d3fd51025b22d85\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at vinai/phobert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /home/kiethoang/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/vocab.txt from cache at /home/kiethoang/.cache/huggingface/transformers/970c6224b2713c8b52a7bcfc4d5a951c9bb88302e4523388b50f28284e87ac44.26ba0c8945e559c68d0bc35d24fea16f5463a49fe8f134e0c32261d590b577fa\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/bpe.codes from cache at /home/kiethoang/.cache/huggingface/transformers/f3a66ae0a78d1a53b3eb99e31837d0d8e2f684a2dcc1f52f75fd36873e3d79de.301ac8958de708ddcea8500d9acbe6261dba391d249c98dcda1e49dbbff870dd\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading file https://huggingface.co/vinai/phobert-base/resolve/main/tokenizer.json from cache at None\n",
      "loading configuration file https://huggingface.co/vinai/phobert-base/resolve/main/config.json from cache at /home/kiethoang/.cache/huggingface/transformers/a596f267f08b7158c7ab6300b1bf98eb6e1b05e6bcb0d7c18a8070364ee3011b.bbe27b2cac909b2279c83792c2d2b6f159f0a95f5d1c1eb66451da1c89a53609\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 258,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"PhobertTokenizer\",\n",
      "  \"transformers_version\": \"4.12.5\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" MLM TESTING \"\"\"\n",
    "\n",
    "# Load pretrained model\n",
    "emandai_model = AutoModelForMaskedLM.from_pretrained(\"./results/TAPT/checkpoint-2500\")\n",
    "vinai_model = AutoModelForMaskedLM.from_pretrained(phobert_base_checkpoint)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(phobert_base_checkpoint, use_fast=False)\n",
    "\n",
    "emandai_unmasker = pipeline(\"fill-mask\", model=emandai_model, tokenizer=tokenizer)\n",
    "vinai_unmasker = pipeline(\"fill-mask\", model=vinai_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17096dac-cda9-4186-af01-437ff9e1f424",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=64001, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emandai_model.eval()\n",
    "vinai_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "df71ec42-3150-4b10-bbde-6306aea2e907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c thÃªm'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" MLM TESTING \"\"\"\n",
    "\n",
    "text = _ws(\"láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c thÃªm\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9212e4d2-9d7d-4ec0-be4c-6e1698299dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c thÃªm',\n",
       "  'score': 0.6051039099693298,\n",
       "  'token': 143,\n",
       "  'token_str': 't h Ãª m'},\n",
       " {'sequence': 'láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c Ä‘áº¡i_há»c',\n",
       "  'score': 0.1962745040655136,\n",
       "  'token': 956,\n",
       "  'token_str': 'Ä‘ áº¡ i _ h á» c'},\n",
       " {'sequence': 'láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c ngoáº¡i_ngá»¯',\n",
       "  'score': 0.05282137915492058,\n",
       "  'token': 4408,\n",
       "  'token_str': 'n g o áº¡ i _ n g á»¯'},\n",
       " {'sequence': 'láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c ná»¯a',\n",
       "  'score': 0.03260941058397293,\n",
       "  'token': 348,\n",
       "  'token_str': 'n á»¯ a'},\n",
       " {'sequence': 'láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c nÆ°á»›c_ngoÃ i',\n",
       "  'score': 0.01431557722389698,\n",
       "  'token': 516,\n",
       "  'token_str': 'n Æ° á»› c _ n g o Ã  i'}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emandai_unmasker(\"láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5c14d98-f42f-4df9-86fe-8cde19371742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c.',\n",
       "  'score': 0.9702707529067993,\n",
       "  'token': 5,\n",
       "  'token_str': '.'},\n",
       " {'sequence': 'láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c :',\n",
       "  'score': 0.013175510801374912,\n",
       "  'token': 27,\n",
       "  'token_str': ':'},\n",
       " {'sequence': 'láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c ;',\n",
       "  'score': 0.006559078581631184,\n",
       "  'token': 65,\n",
       "  'token_str': ';'},\n",
       " {'sequence': 'láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c...',\n",
       "  'score': 0.006110189016908407,\n",
       "  'token': 135,\n",
       "  'token_str': '...'},\n",
       " {'sequence': 'láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c?',\n",
       "  'score': 0.00104904908221215,\n",
       "  'token': 114,\n",
       "  'token_str': '?'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vinai_unmasker(\"láº¥y tiá»n gá»­i cho chÃ¡u Ä‘i há»c <mask>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19501885-d063-464a-95b5-07b3484ce318",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SVM BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "630f8864-effd-4e48-b433-7af96c49ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pformat\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b4b7b99-e17b-48c9-978a-7b1d727e38d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.733\n",
      "\n",
      "Best params:\n",
      " {'clf__estimator__C': 0.1,\n",
      " 'clf__estimator__class_weight': 'balanced',\n",
      " 'vect__max_df': 0.5,\n",
      " 'vect__ngram_range': (1, 1)}\n",
      "\n",
      "Best estimator:\n",
      " Pipeline(steps=[('vect', TfidfVectorizer(max_df=0.5)),\n",
      "                ('scaler', MaxAbsScaler()),\n",
      "                ('clf',\n",
      "                 OneVsRestClassifier(estimator=LinearSVC(C=0.1,\n",
      "                                                         class_weight='balanced',\n",
      "                                                         random_state=1337),\n",
      "                                     n_jobs=-1))])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kiethoang/.local/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"vect\", TfidfVectorizer()),\n",
    "    (\"scaler\", MaxAbsScaler()),\n",
    "    (\"clf\", OneVsRestClassifier(LinearSVC(random_state=1337), n_jobs=-1))\n",
    "])\n",
    "grid = [{\n",
    "        \"vect__ngram_range\": [(1, 1), (1, 2), (2, 2)],\n",
    "        \"vect__max_df\": [0.2, 0.5, 0.75, 1.0],\n",
    "        \"clf__estimator__C\": [0.01, 0.1, 1, 10],\n",
    "        \"clf__estimator__class_weight\": [\"balanced\", None]\n",
    "}]\n",
    "\n",
    "gridcv = GridSearchCV(estimator=pipeline,\n",
    "                      param_grid=grid,\n",
    "                      cv=5,\n",
    "                      scoring=\"f1_macro\",\n",
    "                      n_jobs=-1,\n",
    "                      return_train_score=True)\n",
    "gridcv.fit([*train_texts, *val_texts], [*train_labels, *val_labels])\n",
    "\n",
    "# Statistic verbose\n",
    "print(f\"Best score: {gridcv.best_score_:.3f}\", end=\"\\n\\n\")\n",
    "print(f\"Best params:\\n {pformat(gridcv.best_params_)}\", end=\"\\n\\n\")\n",
    "print(f\"Best estimator:\\n {pformat(gridcv.best_estimator_)}\", end=\"\\n\\n\")\n",
    "\n",
    "linear_svm = gridcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fc1950c-7ba2-4b6f-98e4-33a6410cd85a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 macro [TEST SET]: 0.6370415684180867\n"
     ]
    }
   ],
   "source": [
    "def _evaluate(model, X, y):\n",
    "    y_preds = model.predict(X)\n",
    "    f1_scores = f1_score(y, y_preds, average=\"macro\")\n",
    "    return f1_scores\n",
    "\n",
    "# Evalute on dev set\n",
    "# preds = linear_svm.predict(val_texts)\n",
    "# f1_scores = f1_score(val_labels, preds, average=\"macro\")\n",
    "# dev_f1_score = _evaluate(linear_svm, val_texts, val_labels)\n",
    "# print(f\"F1 macro [DEV SET]: {dev_f1_score}\")\n",
    "\n",
    "# Evalute on test set\n",
    "test_f1_score = _evaluate(linear_svm, test_texts, test_labels)\n",
    "print(f\"F1 macro [TEST SET]: {test_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60fc83c7-7520-4d3b-96a7-ba8728bed220",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 macro [REAL TEST SET]: 0.3203896073664704\n"
     ]
    }
   ],
   "source": [
    "# Evalute on REAL test set\n",
    "real_test_f1_score = _evaluate(linear_svm, X_test, y_test)\n",
    "print(f\"F1 macro [REAL TEST SET]: {real_test_f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf300b4-94d5-4db9-b910-6921e50c6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CONFUSION MATRIX [SVM] \"\"\"\n",
    "\n",
    "y_preds = linear_svm.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_preds)\n",
    "\n",
    "print(len(np.unique(y_preds)))\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5e6a8-2a1d-4955-b921-fef70f21a04a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
