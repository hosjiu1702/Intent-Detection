{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc1999fe-ccd4-41d2-9eed-7e346490c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Text, Tuple, Union\n",
    "from pprint import pprint\n",
    "\n",
    "from pyvi import ViTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, FunctionTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from emandai.utils import load_data_from_botid\n",
    "from tqdm import tqdm\n",
    "\n",
    "from va.src.utils.data import load_va_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fb98aa-abfc-4d96-9255-369286ba5de3",
   "metadata": {},
   "source": [
    "# First Look"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf03662-5321-4e02-a7bf-d6f6534a3a40",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc251d40-b617-49b8-b398-10c96723a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/trungquan/trainset.xlsx\"\n",
    "X, y = load_va_data(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb7cb53-b45c-40d9-b616-a80d33f6d97f",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "631bca38-2e37-455f-95e1-ba99d460adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    \"\"\" Word Segmentation \"\"\"\n",
    "    tokenize = lambda x: ViTokenizer.tokenize(x)\n",
    "\n",
    "    return [tokenize(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "555553c3-fe19-4069-ab5d-43baccdc8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8b3d3-76d4-4967-bb03-2e5ff7840597",
   "metadata": {},
   "source": [
    "## Encode Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4704a4d1-7ad4-4c82-a8dd-eadba7799086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['C2A_TUVAN', 'C2B_GLS', 'C3_KQT'], dtype='<U9')\n",
      "array([[0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       [0, 0, 1],\n",
      "       ...,\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0],\n",
      "       [1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(y)\n",
    "pprint(lb.classes_)\n",
    "pprint(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "124c7c0d-cc24-465c-b1d3-796ee9787dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_label(labels):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(labels)\n",
    "    return lb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b5a1bb-dad2-40f0-84b3-e49e2245a063",
   "metadata": {},
   "source": [
    "## Text Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a555c-a72a-4ed7-b92e-ef047e34aeb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f64c314a-268d-45a3-9840-75af36da9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c5b6c4d8-3de2-48b5-9a06-9b9c6e6f2ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_X = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e466d360-3ad7-431b-a479-b89a9f38d74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1281"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d60b9-2b81-4b88-8c00-a031205ae551",
   "metadata": {},
   "source": [
    "### Vietnamese SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "fa92572c-7791-4f9c-a97c-d363b0218a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SentenceTransformer('keepitreal/vietnamese-sbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f5a4f623-4549-4f13-bfe7-fc6f53a77731",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_X = sbert.encode(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "4e29e8cd-6582-453b-8ad4-d492c8d7cb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396, 768)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0976f64-8b94-45a9-a261-c01c84e7cbad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "56218750-3216-4f6e-b9c0-edc9dbc1f806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "phobert = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\")\n",
    "photokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "163a0b91-8a61-4bb6-8c61-dda38ec4a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phobert_encode(text: Union[Text, List[Text]],\n",
    "                   phobert,\n",
    "                   photokenizer) -> np.ndarray:\n",
    "    \"\"\" Get sentence embedding from [CLS] token of the given text.\n",
    "    reference: https://discuss.huggingface.co/t/how-to-get-cls-embeddings-from-bertfortokenclassification-model/9276/3\n",
    "    \"\"\"\n",
    "    embeddings = None\n",
    "\n",
    "    encoded_text = photokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    text_ids = encoded_text[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        outputs = phobert(text_ids, output_hidden_states=True)\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        embeddings = last_hidden_state[:, 0, :]\n",
    "\n",
    "        if isinstance(embeddings, torch.Tensor):\n",
    "            embeddings = embeddings.detach().cpu().numpy()\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7d53d48-e200-4683-ba21-f036995153a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "phobert_X = phobert_encode(X, phobert, photokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01ee758d-f76e-43cf-94f8-df662315a1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396, 768)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phobert_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23039145-ad71-4340-be63-22e1ae8bf931",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d69ee9-2ef0-4fde-a1dc-3ff091919f5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "370f799e-ea87-44a0-8cc7-6d026875e001",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buihuy/Intent-Detection/venv/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/buihuy/Intent-Detection/venv/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/home/buihuy/Intent-Detection/venv/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "clf = {}\n",
    "clf[\"tfidf\"] = OneVsRestClassifier(LogisticRegression()).fit(tfidf_X, y)\n",
    "clf[\"sbert\"] = OneVsRestClassifier(LogisticRegression()).fit(sbert_X, y)\n",
    "clf[\"phobert\"] = OneVsRestClassifier(LogisticRegression()).fit(phobert_X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d7416-ac7d-4a76-8565-0fd9a99f4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Inference \"\"\"\n",
    "def infer(text: Text,\n",
    "          embedding_type: Text = \"tfidf\",\n",
    "          with_prob: bool = False\n",
    "    ) -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n",
    "    pred = []\n",
    "\n",
    "    if not isinstance(text, List):\n",
    "        text = [text]\n",
    "\n",
    "    if embedding_type == \"tfidf\":\n",
    "        vectorizer = tfidf\n",
    "        _clf = clf[\"tfidf\"]\n",
    "    elif embedding_type == \"sbert\":\n",
    "        vectorizer = sbert\n",
    "        _clf = clf[\"sbert\"]\n",
    "    elif embedding_type == \"phobert\":\n",
    "        vectorizer = phobert\n",
    "        _clf = clf[\"phobert\"]\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # preprocess\n",
    "    text = preprocess(text)\n",
    "\n",
    "    # vectorize\n",
    "    if embedding_type == \"tfidf\":\n",
    "        features = vectorizer.transform(text)\n",
    "    elif embedding_type == \"sbert\":\n",
    "        features = vectorizer.encode(text)\n",
    "    elif embedding_type == \"phobert\":\n",
    "        features = phobert_encode(text)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    # classify\n",
    "    pred = _clf.predict(features)\n",
    "\n",
    "    if with_prob:\n",
    "        prob = _clf.predict_proba(features)\n",
    "        prob = np.max(prob, axis=1)\n",
    "        return pred, prob\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e560c8b5-ebd8-4e83-a665-684a6c1e2227",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec528a-26ad-46d8-b2c6-c45d194336f0",
   "metadata": {},
   "source": [
    "##### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5bd7c0c8-debe-4da9-b3b9-2596e019d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1(text):\n",
    "    pred, prob = infer(text, embedding_type=\"tfidf\", with_prob=True)\n",
    "    pprint(pred)\n",
    "    pprint(lb.inverse_transform(pred))\n",
    "    pprint(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "5d0a2944-a227-41d1-ba41-354e71aee0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 0]])\n",
      "array(['C2A_TUVAN'], dtype='<U9')\n",
      "array([0.34704396])\n"
     ]
    }
   ],
   "source": [
    "text = \"em có thể diễn giải chi tiết hơn được không\"\n",
    "test1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "85e6eea7-d35d-4e93-96a6-62e3992082f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 1, 0]])\n",
      "array(['C2B_GLS'], dtype='<U9')\n",
      "array([0.51182048])\n"
     ]
    }
   ],
   "source": [
    "text = \"hay là chút nữa liên lạc lại em nhé\"\n",
    "test1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6f3c9d75-fe15-4bdf-a57c-df2d44a47bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 1]])\n",
      "array(['C3_KQT'], dtype='<U9')\n",
      "array([0.62048898])\n"
     ]
    }
   ],
   "source": [
    "text = \"cảm ơn em nhưng chắc là anh không hứng thú\"\n",
    "test1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "0050c9a8-0744-4974-9d2c-5c56e7079cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 0]])\n",
      "array(['C2A_TUVAN'], dtype='<U9')\n",
      "array([0.46300921])\n"
     ]
    }
   ],
   "source": [
    "text = \"anh nghĩ là em nên dừng lại\"\n",
    "test1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "4bbd9ecf-e1c2-4023-bb2a-fab0e660ba02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 0]])\n",
      "array(['C2A_TUVAN'], dtype='<U9')\n",
      "array([0.46125805])\n"
     ]
    }
   ],
   "source": [
    "text = \"dạ không ạ\"\n",
    "test1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "190db567-71f9-43ca-9e38-df7a5566c8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 0]])\n",
      "array(['C2A_TUVAN'], dtype='<U9')\n",
      "array([0.36902174])\n"
     ]
    }
   ],
   "source": [
    "text = \"không đâu chị ơi\"\n",
    "test1(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d26debd3-57d9-4b22-a7c8-84d1c92168a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 0]])\n",
      "array(['C2A_TUVAN'], dtype='<U9')\n",
      "array([0.38223752])\n"
     ]
    }
   ],
   "source": [
    "text = \"ừ không, mình đang làm rồi bạn nha\"\n",
    "test1(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5729f50-f058-4a22-8807-b0279c730260",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "91c06266-377a-46f8-aaec-472e051c4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2(text):\n",
    "    pred, prob = infer(text, embedding_type=\"sbert\", with_prob=True)\n",
    "    pprint(pred)\n",
    "    pprint(lb.inverse_transform(pred))\n",
    "    pprint(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "6d44e1f3-bf2e-48b1-a943-6e5b59828a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1, 0, 0]])\n",
      "array(['C2A_TUVAN'], dtype='<U9')\n",
      "array([0.69315568])\n"
     ]
    }
   ],
   "source": [
    "text = \"em có thể diễn giải chi tiết hơn được không\"\n",
    "test2(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4eda13b3-ba7f-481d-8a32-53a587f1ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 1, 0]])\n",
      "array(['C2B_GLS'], dtype='<U9')\n",
      "array([0.95827348])\n"
     ]
    }
   ],
   "source": [
    "text = \"hay là chút nữa liên lạc lại em nhé\"\n",
    "test2(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "102c3afa-1753-4d12-a63d-b1f2ae85fb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 1]])\n",
      "array(['C3_KQT'], dtype='<U9')\n",
      "array([0.77097445])\n"
     ]
    }
   ],
   "source": [
    "text = \"cảm ơn em nhưng chắc là anh không hứng thú\"\n",
    "test2(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e240515a-a4c4-4866-a705-a70c460df720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 0]])\n",
      "array(['C2A_TUVAN'], dtype='<U9')\n",
      "array([0.21407058])\n"
     ]
    }
   ],
   "source": [
    "text = \"anh nghĩ là em nên dừng lại\"\n",
    "test2(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ef39d785-a004-426d-983a-9c6df7f20c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1, 0, 0]])\n",
      "array(['C2A_TUVAN'], dtype='<U9')\n",
      "array([0.89481114])\n"
     ]
    }
   ],
   "source": [
    "text = \"dạ không ạ\"\n",
    "test2(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "3f9c199b-cc4a-4bad-9e6f-d21f26d972fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 1]])\n",
      "array(['C3_KQT'], dtype='<U9')\n",
      "array([0.66558446])\n"
     ]
    }
   ],
   "source": [
    "text = \"không đâu chị ơi\"\n",
    "test2(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "4853b06c-98fe-4f2a-932b-7b50a9e7529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 1]])\n",
      "array(['C3_KQT'], dtype='<U9')\n",
      "array([0.80773914])\n"
     ]
    }
   ],
   "source": [
    "text = \"ừ không, mình đang làm rồi bạn nha\"\n",
    "test2(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b19c4-3ecc-4d9c-94f7-47c639ba4647",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "##### PhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "a3c8a690-46a5-41b2-a6d6-807af2566a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test3(text):\n",
    "    pred, prob = infer(text, embedding_type=\"phobert\", with_prob=True)\n",
    "    pprint(pred)\n",
    "    pprint(lb.inverse_transform(pred))\n",
    "    pprint(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "21d34ebe-463b-4b32-ab17-8c8768dc8ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1, 0, 0]])\n",
      "array(['C2A_TUVAN'], dtype='<U9')\n",
      "array([0.59992224])\n"
     ]
    }
   ],
   "source": [
    "text = \"em có thể diễn giải chi tiết hơn được không\"\n",
    "test3(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "8db496e5-685a-4d3d-98bc-4c3e8a511855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 0, 0]])\n",
      "array(['C2A_TUVAN'], dtype='<U9')\n",
      "array([0.37629714])\n"
     ]
    }
   ],
   "source": [
    "text = \"hay là chút nữa liên lạc lại em nhé\"\n",
    "test3(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82528a4-095c-4c69-9bf9-0cff24b7240d",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076000b-7241-4d4a-907c-0a5df346742a",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "750bc2ed-47d4-4054-af75-d1580135997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "botid = {\n",
    "    \"sunlaw\": \"6268f7e49f455cd4ea292d88\",\n",
    "    \"lamhaian\": \"6268f8049f455c653b292e29\",\n",
    "    \"giang.nguyen\": \"6268f7f89f455cd9eb292df4\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f430b4-7a64-4e11-803e-03503767a58a",
   "metadata": {},
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "060648c1-cb11-4b2d-98fd-940fc3ff7502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n",
      "INFO:oauth2client.transport:Attempting refresh to obtain initial access_token\n",
      "INFO:oauth2client.client:Refreshing access_token\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = {k: load_data_from_botid(v) for k, v in botid.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f67275f-159b-493e-94a5-29903d844494",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d9c70-3fa9-4a51-ba03-09afaaff0bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c9520fb-c3e6-474a-824c-b8d6daf16b35",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "929d7169-daa6-44df-8781-40c75da3b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "def preprocess(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\" Text preprocessing\n",
    "    \"\"\"\n",
    "    # remove duplicate\n",
    "    data = data.drop_duplicates(\"Sentence\")\n",
    "\n",
    "    # lower\n",
    "    data[\"Sentence\"] = data[\"Sentence\"].map(lambda x: x.lower())\n",
    "    data.head(n=10)\n",
    "\n",
    "    # word segmenation\n",
    "    data[\"Sentence\"] = data[\"Sentence\"].map(ViTokenizer.tokenize)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3e2a348e-13db-48dd-b011-a7a4a48e17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = {k: preprocess(v) for k, v in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a5cba-9a0e-4ca3-a68e-cd4fccd08484",
   "metadata": {},
   "source": [
    "## Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "22cf42a6-355f-4d6f-82e3-db65ff00a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: keepitreal/vietnamese-sbert\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Inititialize featurizers \n",
    "\"\"\"\n",
    "tfidf = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", ngram_range=(1, 2))\n",
    "sbert = SentenceTransformer('keepitreal/vietnamese-sbert')\n",
    "phobert = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\")\n",
    "photokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "981107df-e79c-4a12-a888-7df865340564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(X: Union[Text, List[Text]],\n",
    "                 embedding_type: Text) -> np.ndarray:\n",
    "    if embedding_type == \"tfidf\":\n",
    "        X = tfidf.fit_transform(X).toarray()\n",
    "    elif embedding_type == \"sbert\":\n",
    "        X = sbert.encode(X)\n",
    "    elif embedding_type == \"phobert\":\n",
    "        X = phobert_encode(X, phobert, photokenizer)\n",
    "    else:\n",
    "        raise ValueError(\"Current supported text encoders: ['tfidf', 'sbert', 'phobert']\")\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b63e5a20-7107-470a-b318-b1cf04ebd603",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = {k: v[\"Sentence\"].tolist() for k, v in preprocessed_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13a50c8b-dd00-40fa-b5d0-8960db5da595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1db896c0324dba93e8dea78c19e78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████████████████                                                                  | 1/3 [00:05<00:10,  5.07s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac690a5a6d5745029c87dead539e22ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████████████████████████████████████████████                                 | 2/3 [00:09<00:04,  4.47s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48c727f5e0c46878598712c8ad9eb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.74s/it]\n"
     ]
    }
   ],
   "source": [
    "embedding_types = [\"tfidf\", \"sbert\", \"phobert\"]\n",
    "features = {k: {t: get_features(X[k], t) for t in embedding_types} for k, v in tqdm(X.items())}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eeffc2-ac7c-498c-96ca-0c3dc2894883",
   "metadata": {},
   "source": [
    "## Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "615581d7-9f3b-49a7-b51b-3c50ed2aabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: [row[row == 1].index.tolist() for _, row in x.iterrows()]\n",
    "y = {k: f(v) for k, v in preprocessed_data.items()}\n",
    "label_encoders = {k: MultiLabelBinarizer().fit(v) for k, v in y.items()}\n",
    "y = {k: label_encoders[k].transform(v) for k, v in y.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f9692-29c8-492e-8d98-bfead127a012",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b981ca-b016-497b-b0e0-293513bfc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(embs_type):\n",
    "    if embs_type == \"tifdf\":\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (\"vect\", tfidf),\n",
    "            (\"clf\", LogisticRegression())\n",
    "        ])\n",
    "    elif embs_type == \"sbert\":\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (\"vect\", FunctionTransformer(sbert.encode)),\n",
    "            (\"clf\", LogisticRegression())\n",
    "        ])\n",
    "    elif embs_type == \"phobert\":\n",
    "        pipeline = Pipeline(steps=[\n",
    "            (\"vect\", FunctionTransformer()),\n",
    "            (\"clf\", LogisticRegression())\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"Current supported text encoders: ['tfidf', 'sbert', 'phobert']\")\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e758d4d-1e5a-40f3-8eb7-7d3cb8ac1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {for k, v in }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affb0093-8e04-43e2-a819-4190a47bb733",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f3830-0c94-4b21-9d85-579d0c9dff47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
